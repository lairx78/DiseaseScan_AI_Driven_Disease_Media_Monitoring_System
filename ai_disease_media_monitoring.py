# -*- coding: utf-8 -*-
"""AI_Disease_Media_Monitoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16pgJ_UNy8YjpnbcXuUQFIapfGy031BO6
"""

import sqlite3
import pandas as pd
import contextlib
import io
import os
from datetime import datetime
import uuid
import shutil

import analysis_agent
from analysis_agent import run_conversation

def create_db_and_table(db_name="etl.db"):
    """
    Creates the SQLite database and the 'articles' table with fixed columns.
    The columns are:
      id, disease_related, article_title, disease_names, summary.
    """
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY,
            disease_related TEXT,
            article_title TEXT,
            disease_names TEXT,
            summary TEXT
        );
    """)
    conn.commit()
    conn.close()
    print("Database and table 'articles' created successfully.")

def insert_or_update_record(id_value, data, db_name="etl.db"):
    """
    Inserts or updates a record in the 'articles' table based on the id.

    - If the record doesn't exist, it is inserted.
    - If the record exists, all allowed columns are overwritten:
      for any column not provided in the update dictionary,
      its value will be set to NULL.

    If 'disease_names' in the data is a list (e.g., ["ABC", "DEF"]),
    it is converted to a comma-separated string ("ABC, DEF") before storage.
    """
    # Convert disease_names from list to comma-separated string if necessary.
    if "disease_names" in data:
        if isinstance(data["disease_names"], list):
            data["disease_names"] = ", ".join(data["disease_names"])

    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()

    # Check if a record with the given id exists.
    cursor.execute("SELECT COUNT(*) FROM articles WHERE id = ?", (id_value,))
    exists = cursor.fetchone()[0] > 0

    if not exists:
        # Insert new record.
        columns = ["id", "disease_related", "article_title", "disease_names", "summary"]
        # For insert, any missing key becomes None.
        values = [id_value] + [data.get(col, None) for col in columns[1:]]
        placeholders = ", ".join(["?"] * len(columns))
        sql_query = f"INSERT INTO articles ({', '.join(columns)}) VALUES ({placeholders})"
        print("Inserting record. SQL Query:")
        print(sql_query)
        print("Values:", values)
        cursor.execute(sql_query, values)
    else:
        # Update existing record. The allowed columns are overwritten.
        allowed_columns = ["disease_related", "article_title", "disease_names", "summary"]
        update_parts = []
        update_values = []
        for col in allowed_columns:
            update_parts.append(f"{col} = ?")
            # Use data value if supplied; otherwise set to None.
            update_values.append(data.get(col, None))
        sql_update = f"UPDATE articles SET {', '.join(update_parts)} WHERE id = ?"
        update_values.append(id_value)
        print("Updating record. SQL Query:")
        print(sql_update)
        print("Values:", update_values)
        cursor.execute(sql_update, update_values)

    conn.commit()
    conn.close()
    print(f"Data with id {id_value} processed successfully.\n")

def table_as_dataframe(db_name="etl.db"):
    """
    Reads the entire 'articles' table into a Pandas DataFrame and prints it.
    """
    conn = sqlite3.connect(db_name)
    df = pd.read_sql_query("SELECT * FROM articles", conn)
    conn.close()
    # print("\nDataFrame of table 'articles':")
    # print(df)
    return df

def AI_Disease_Media_Monitoring(news_excel="all_new.xlsx", db_to_excel=False, db_name="etl.db", agent_message=False):
  """
  This is a system for:
  1. Classify news content as disease-related or not.
 2. Extract key disease entity names.
 3. Summarize the nuanced information from news articles.
 4. Generate comprehensive word reports and PowerPoint presentations from
  5. scraped HTML content.
  """

  all_news_df = pd.read_excel("all_news.xlsx")
  news_id_lists = all_news_df["news_id"].tolist()
  news_source_name_lists = all_news_df["news_source_name"].tolist()

  original_directory = os.getcwd()
  now = datetime.now()
  new_folder_name = now.strftime("%Y%m%d_%H%M%S")

  if not os.path.exists(new_folder_name):
    os.makedirs(new_folder_name)

  os.chdir(new_folder_name)
  create_db_and_table(db_name)

  for filename in news_source_name_lists:
    if filename.lower().endswith(('.htm', '.html')):

      source_file_path = os.path.join(original_directory , filename)
      destination_file_path = os.path.join(original_directory, new_folder_name, filename)

      json_path = os.path.join(original_directory , "project78-456610-b3f1efc8ee09.json")
      new_json_path = os.path.join(original_directory, new_folder_name, "project78-456610-b3f1efc8ee09.json")

      if os.path.isfile(source_file_path):
        shutil.copy(source_file_path, destination_file_path)
        shutil.copy(json_path, new_json_path)

  df_rows = len(all_news_df.index)
  for i in range(df_rows):

    conversation_thread_id = str(uuid.uuid4())
    if agent_message==False:
      with contextlib.redirect_stdout(io.StringIO()):
        result = run_conversation(news_source_name_lists[i], conversation_thread_id)
        insert_or_update_record(news_id_lists[i], analysis_agent.result_dict, db_name)
    elif agent_message==True:
      result = run_conversation(news_source_name_lists[i], conversation_thread_id)
      insert_or_update_record(news_id_lists[i], analysis_agent.result_dict, db_name)

  df_etl = table_as_dataframe(db_name)
  display(df_etl)

  if db_to_excel==True:
    df_etl.to_excel("etl.xlsx")

  for file in os.listdir('.'):
    if file.lower().endswith(('.htm', '.html')):
        file_path = os.path.join(os.getcwd(), file)
        if os.path.isfile(file_path):
            os.remove(file_path)

  os.remove("project78-456610-b3f1efc8ee09.json")
  os.chdir(original_directory)